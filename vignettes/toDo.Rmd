---
title: "ToDos"
author: "Cory Merow"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{rangeModelMetadata R package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
***
#-1. Overarching goal

* moving from potential distribution toward more like a realized distribution
* better estimates of range size
* Using expert-type information to refine (mask) range models that wouldn't otherwise fit in a statistical framework.
* for conservation applications
* getting to improved biodiversity estimates when some species are data poor

# 0. Getting started

* need a few example species
* put code for any bugs in this document because compiling here will be sure we're working with the same code and getting the same reproducible errors
* use camelCase throughout
* hide internal functions with .
* we can offer 2 types of masking: deterministic and stochastic (via range bagging)
* CM: do we need to impose a rule that you can only have a continuous map at 1 stage and this gets masked. or is it meaningful to start with a binary map and make it continuous via range bagging?
* if we clean up this doc after we resolve problems, then it's pretty much the software note. 
* because we're clipping you can't do forecasting. (well, i guess you can do bounds)
    + because we're refining the realized distribution, there's no point in forecasting
* let's make sure we're not redoing raster operations or apis that need maintainence. data prep is in wallace.
* are there applications where we assign probabilities or proportions. e.g. proportion cover in LU scenarios. any other scenarios like this?
* start with global maps that are already aligned. 


# 0.5 Use cases

## Intended application
* blob map of another species
* south african plant with soil specificity (serpentine)

## Data available
* user-specified rasters that are poorly aligned

## Expert Driven
* Expert map is the starting SDM
* Few observations and you already know a threshold function is the relationship

## Data driven 
* use all historical + modern data for SDM to start with. use modern data with data  driven approach for clipping. 
    + historic records include env conditions that new records won't include because we trashed earth nonrandommly
* SDM model was built somewhere else and isn't reproducible
    + someone made an SDM in the past that’s not reproducible so you treat it as a blob
* maybe this is appropriate if the starting point is a nonstatistical model, e.g., an IUCN map

## Species examples


<!--------------------------------------------------------------->
<!--------------------------------------------------------------->
***
# 1. Modes to Clip

## A. Expert opinion
  
+ Potential distribution (SDM) 
+ RS layers to cut with
+ Expert opinion

* 1 function
    * inputs: potential distribution raster, list of RS rasters, list of lower bounds, in same order as RS rasters, list of upper bounds in same order as RS rasters
    * assumes binary masking (not adjusting probabilities)
  
```{r}
#' @param potentialDist A raster stack of binary or continuous values. Supplying more than one layer will be interepreted as different time periods. Layers should follow the naming convention `Y2000`, `Y2001`, etc.
#' @param rsList A list of raster stacks. Each layer in a stack corresponds to a a different time period; objects of class `raster` or stacks with one layer will be interpreted as a single time period. Note that this is a list to enable layers with different resolutions or extents to be used.
#' @param rsLower A list of lower bounds of suitable values in the same order as `rsList`
#' @param rsUpper A list of upper bounds of suitable values in the same order as `rsList`
#' @param method A list of strings defining methods to be used, in the same order as `rsList`. If a single value is provided it will be applied to all rasters in `rsList`. Options include:
#' \itemize{
#'  \item{mask}{mask cells with values outside the bounds}
#'  \item{"parameter 2"}{Stuff}
#' }
# would we ever have the case where you'd have suitable < X < Y< suitable?
maskRS=function(potentialDist,rsList,rsLower,rsUpper,method){
  #Cory will do
  return(realizedDist)
}
```

> CM: I think this can be applied successively if you want too. That is, the output from one call can be used as the input for another call. 
  

## B. Data driven

> CM: I'm not entirely sure what's meant by this section. Is the idea that you use the presences to choose a threshold for what should be masked? If so, we may need to think about the appropriate use cases. Its kind of like fitting 2 step functions to data for this covariate either instead of a statistical model (i.e. you start with an expert map) or outside of a statisical model (kind of cheating). If you start with an expert map, why not use my model from GEB? And if you start with a statistical model, why not fit a function to this covariate (you lose any meaning in the uncertainty if you split the model into 2 parts)? 

+ maybe this is appropriate if the starting point is a nonstatistical model
    - non coding version of by GEB model
    - maybe see if you can pull these from IUCN api
+ someone made an SDM in the past that's not reproducible so you treat it as a blob.
+ data driven case is only for when there are too few points for a statistical model
+ maybe the scope is that you shouldn't be building models with >30 points per covariates
+ Potential distribution (SDM)
+ RS layers to cut with
+ Occurrence records (that correspond to time periods of layers) – spatio-temporal matching, convex hulls or quantiles of distribution
    - pull with `spocc::occ` or `BIEN::BIEN`
+ use the same function from A. above to do the masking, but use data to determine the upper and lower bounds

<!--------------------------------------------------------------->
<!--------------------------------------------------------------->

***
# 2. Data Needed

> CM: I don't really think we need to write any functions for this part. people should be able to read in their own data.

## Shapefiles

+ Categorial (vegetation classes, geology, protected areas, threats)
    * are we pulling these data in from an api? 
        - does protectedareas.org have an api? would be nice, but they have massive duplication in their layers and merging them is a huge mission. i have them at 1km, but not sure we'd be allowed to reserve that.
    * user provided 

## Grids

+ Categorical
    * are we pulling these data in from an api? 
        - soilgrids.org? maybe an api
+ Continuous
    * are we pulling these data in from an api? 
        - worldclim via dismo? i guess you might have upper an lower rainfall limits for plants from a field guide.
    * upper tail, lower tail or both
        - CM: what does this mean?
    * %ile around threshold value
        - CM: what does this mean?

<!--------------------------------------------------------------->
<!--------------------------------------------------------------->
***
# 3. Methods

These are all ways to come up with bounds or the logical criterion. Need to think of a more flexible way to specify the logic rather than just upper and lower bounds.

**Clip by pixel vs neighborhood (Radius – spatial & temporal)**

* area still suitable
* sounds like this radius is for a point, not a grid. so would you provide points, and we'd buffer them, make a grid, and use the mask function?
* what are the use cases for buffers in time? this will mean that we have to have stacks rather than rasters 
    
**Clip by landscape statistics (edgeness, patch size, roads, etc) – closer to “area occupied”**

* does this just involve make a raster of these and using `method='mask'`

**Clip out competitor/ other  species**

* this is just a simple mask, handled by `method='mask'` 

**Clip by threat or PA maps** 

* this is just a simple mask, handled by `method='mask'` 

**Spatio-temporal Trends vs. “Most Recent” – calculate for all years in series or most recent** 

* this is just a simple mask, handled by `method='mask'` 
* sounds like this is a buffered point, as with the neighborhood approach above.
* challenge is that the time series of the mask must align with the time series of the species map. i guess you could name each layer 'Y2001', 'Y2002', etc. and match species maps and masks by year

**Range Bagging**

* useful for data driven approach if you want a continuous prediction. randomly sample rs layers, randomly sample presences, build convex hull in env space, throw on a map.
* [paper](http://rsif.royalsocietypublishing.org/content/12/107/20150086)

<!--------------------------------------------------------------->
<!--------------------------------------------------------------->
***
# 4. Sensitivity Analysis

* Compare diff methods 
    + CM: at this point, i just have this as stochastic and stochastic
    + I guess if you're buffering points you could compare different buffers
* Spatial and temporal sensitivity
    + CM: not sure what this means
* Ensemble or choose best one
    + CM: how are we evaluating these? Are we doing a full SDM modeling workflow? I was under the impression that we're taking a non-statistical approach to incorporating other knowledge (e.g. elevation limits from a field guide)

<!--------------------------------------------------------------->
<!--------------------------------------------------------------->
***
# 5. Output

* Binary
* Threshold-cut continuous
    + CM:what is this?
* 2x2km res for IUCN AOO Upper bound
    + so this is just an equal area reprojection, right?
    + if so, cory hasa function that'll center on the range centroid and will split disjoint ranges into their separate parts to avoid stupid stuff with equal area projections far fromthe centriod.
* can we plot the range area as a function of number of masks used to show that it begins to asympote to the true range, under the **critical** asumption that the masking layers used are relevant?

<!--------------------------------------------------------------->
<!--------------------------------------------------------------->
***
# X. GIS Problems

* Projections
* Resolution
    + allow different extents for different masks to allow for limitations of different data sources
* Extent
    + allow different extents for different masks to allow for limitations of different data sources
* Coincidence (resampling)

> CM: do we want to force reprojecting a mask to the species' map projection or just offer it as an option. or we could allow an argument called `targetProjection` that all the rasters get put into. or should users be required to manage their projectiosn before inputting? we're not trying to reinvent GIS and so we're sort of limiting ourselves if we hard code a basic GIS operation.

<!--------------------------------------------------------------->
<!--------------------------------------------------------------->
***




